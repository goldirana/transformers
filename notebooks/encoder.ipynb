{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"The cat is sleeping on the mat\", \"She loves to read books in the evening\", \"The sun rises in the east and sets in the west\", \n",
    "        \"He bought a new car last week\", \"They are going to the park to play soccer\",\n",
    "        \"The teacher explained the lesson clearly\", \"I enjoy drinking coffee in the morning\",\n",
    "        \"The children laughed at the funny joke\", \"We visited the museum during our vacation\", \n",
    "        \"The dog barked loudly at the stranger\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = None # to store the vocab\n",
    "        self.input_ids = None # to store the input ids\n",
    "        \n",
    "    def get_tokenizer(self, data):\n",
    "        vocab = set()\n",
    "        for sentence in data:\n",
    "            for i in sentence.lower().split(\" \"):\n",
    "                vocab.add(i)\n",
    "        self.vocab = vocab\n",
    "        return self.vocab\n",
    "    \n",
    "    def mapper(self, vocab):\n",
    "        tokens = {}\n",
    "        for i, element in enumerate(vocab):\n",
    "            tokens[element] = i\n",
    "        self.input_ids = tokens\n",
    "        return self.input_ids\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        vocab = self.get_tokenizer(data)\n",
    "        tokens = self.mapper(vocab)\n",
    "        return tokens\n",
    "    \n",
    "    def set_max_len(self, x, max_len):\n",
    "        current_len = len(x)\n",
    "        len_diff = max_len - current_len\n",
    "        \n",
    "        if len_diff <=  max_len and (len_diff >0):\n",
    "            for i in range(0, len_diff):\n",
    "                x.append(0)\n",
    "            return x\n",
    "        else:\n",
    "            return x[0:max_len]\n",
    "            \n",
    "    def transform(self, data, tokens, max_len: int):\n",
    "        _ = []\n",
    "        for sentence in data:\n",
    "            x = []\n",
    "            for word in sentence.lower().split(\" \"):\n",
    "                x.append(tokens[word])\n",
    "            _.append(self.set_max_len(x, max_len = max_len))\n",
    "        return th.tensor(_)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the vocab size\"\"\"\n",
    "        assert self.vocab != None, \"Tokenizer not fit\"\n",
    "        return len(self.vocab)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokens = tokenizer(data)\n",
    "\n",
    "vocab_size = tokenizer.__len__()\n",
    "d_model = 50\n",
    "max_len = 5\n",
    "data = tokenizer.transform(data, tokens, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure to follow:\n",
    "1. Embeddings\n",
    "2. Positional Encoding\n",
    "3. multihead attention\n",
    "4. Residual connection\n",
    "5. Layer normalization\n",
    "6. Encoder Block\n",
    "6.1 Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "    \n",
    "\n",
    "class WithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len: int, d_model:int, vocab_size:int):\n",
    "        super(WithPositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.embdding = EmbeddingLayer(vocab_size, d_model)\n",
    "    \n",
    "    def get_embedding(self):\n",
    "        pe = th.zeros(self.max_len, self.d_model)\n",
    "        index = th.arange(0, self.max_len).unsqueeze(1)\n",
    "        div_term = th.exp(th.arange(0, self.d_model, 2) * -(th.log(th.tensor(10000.0)) / self.d_model))\n",
    "        # for even indices\n",
    "        pe[:, ::2] = th.sin(index*div_term)\n",
    "        pe[:, 1::2] = th.cos(index*div_term)\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[-1]\n",
    "        return self.embdding(x) + self.get_embedding()[:seq_len, :]\n",
    "\n",
    "\n",
    "ob = WithPositionalEncoding(max_len, d_model, vocab_size)    \n",
    "embeddings = ob(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape (50, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Multihead attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # dimnesion of model/embeddings\n",
    "        self.h = h # number of heads\n",
    "        \n",
    "        # to ensure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is nit divisible by h\"\n",
    "        \n",
    "        self.d_k = d_model // h # dimension of vector seen by each head \n",
    "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w = nn.Linear(d_model, d_model, bias=False) # shape = d_model, d_model\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(query, key, value):\n",
    "        d_k = query.shape[-1]\n",
    "        # query/keys shape -> batch, heads, seq_len, dim\n",
    "        # keys shape after transpose -> batch, heads, dim, seq_len\n",
    "        \n",
    "        # attention score - >> batch, heads, seq_len, seq_len\n",
    "        \n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        attention_scores = attention_scores.softmax(dim = -1) # batch, h, seq_len, seq_len\n",
    "        \n",
    "        # values shape -> batch, heads, seq_len, dim\n",
    "        # after matmul with attention score -> batch, heads, seq_len, dim\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = self.q(x) # batch, seq_len, d_model -> batch, seq_len, d_model\n",
    "        keys = self.k(x)  # batch, seq_len, d_model -> batch, seq_len, d_model\n",
    "        values = self.v(x) # batch, seq_len, d_model -> batch, seq_len, d_model\n",
    "        \n",
    "        # batch, seq_len, d_model -> batch, seq_len, h, d_k -> batch, h, sq_len, d_k\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2) # after view shape -> batch, seq_len, heads, dim\n",
    "        keys = keys.view(keys.shape[0], keys.shape[1], self.h, self.d_k).transpose(1, 2) \n",
    "        values = values.view(values.shape[0], values.shape[1], self.h, self.d_k).transpose(1, 2) # after transpose -> batch, heads, seq_len, dim\n",
    "        \n",
    "        # calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, keys, values)\n",
    "        \n",
    "        # combine all heads together\n",
    "        # x shape (attention score with matmul values)-> batch, heads, seq_len, dim  -> batch, seq_len, heads, dim -> batch, seq_len, dim\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # multiply with w -> batch, seq_len, d_model -> batch, seq_len, d_model\n",
    "        return self.w(x)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Residual Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features, eps:float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(th.ones(features))\n",
    "        self.beta = nn.Parameter(th.zeros(features))\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim = -1, keepdim=True) # calculate mean across dim \n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * ((x - mean)/(std + self.eps)) + self.beta\n",
    "    \n",
    "    \n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.norm(x)\n",
    "    \n",
    "    \n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(th.relu(self.linear_1(x))))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.std(-1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 0, 0, 0, 1, 0],\n",
       "         [0, 2, 2, 3, 3, 2, 3],\n",
       "         [0, 2, 0, 3, 1, 1, 0],\n",
       "         [2, 2, 2, 3, 3, 3, 3]],\n",
       "\n",
       "        [[1, 2, 1, 3, 0, 1, 3],\n",
       "         [2, 3, 0, 1, 0, 0, 0],\n",
       "         [0, 1, 2, 2, 0, 3, 2],\n",
       "         [2, 3, 1, 2, 1, 2, 3]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.randint(0, 4, size=(2, 4, 7))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4286],\n",
       "         [2.1429],\n",
       "         [1.0000],\n",
       "         [2.5714]],\n",
       "\n",
       "        [[1.5714],\n",
       "         [0.8571],\n",
       "         [1.4286],\n",
       "         [2.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim = -1, dtype=float, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4286, 1.4286, 0.4286, 0.4286, 0.4286, 1.4286, 0.4286],\n",
       "         [2.1429, 4.1429, 4.1429, 5.1429, 5.1429, 4.1429, 5.1429],\n",
       "         [1.0000, 3.0000, 1.0000, 4.0000, 2.0000, 2.0000, 1.0000],\n",
       "         [4.5714, 4.5714, 4.5714, 5.5714, 5.5714, 5.5714, 5.5714]],\n",
       "\n",
       "        [[2.5714, 3.5714, 2.5714, 4.5714, 1.5714, 2.5714, 4.5714],\n",
       "         [2.8571, 3.8571, 0.8571, 1.8571, 0.8571, 0.8571, 0.8571],\n",
       "         [1.4286, 2.4286, 3.4286, 3.4286, 1.4286, 4.4286, 3.4286],\n",
       "         [4.0000, 5.0000, 3.0000, 4.0000, 3.0000, 4.0000, 5.0000]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + x.mean(dim = -1, dtype=float, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(84)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, torch.Size([2, 4, 7]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel(), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "84/56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.142857142857143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0, 2, 2, 3, 3, 2, 3])/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.v = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape, self.q.weight.shape)\n",
    "        query = self.q(x) # batch_size, seq_length, q.dim-1\n",
    "        keys = self.k(x) \n",
    "        values = self.v(x)\n",
    "        # print(query.shape, keys.shape, values.shape)\n",
    "        attention_score = th.softmax(th.matmul(query, keys.transpose(-1, -2))/ th.sqrt(th.tensor(d_model)), dim=-1)\n",
    "        attention_weight = th.matmul(attention_score, values)\n",
    "        return attention_weight\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ResidualConnection(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layer_norm = LayerNormalization(self.d_model)\n",
    "        self.attention = AttentionHead(self.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "def FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(self.d_model, d_model)\n",
    "        \n",
    "           \n",
    "\n",
    "ob = AttentionHead(50)\n",
    "attention_weight = ob(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
